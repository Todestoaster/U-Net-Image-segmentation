{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f049b8-d980-48ec-8b54-c4ce70a15442",
   "metadata": {},
   "source": [
    "# Image Segmentation with U-Net on the Oxford-IIIT Pet Dataset\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, we will implement an image segmentation model using the U-Net architecture on the Oxford-IIIT Pet Dataset. The notebook includes detailed explanations and comments in the code to facilitate understanding. We will:\n",
    "\n",
    "- Load and preprocess the dataset.\n",
    "- Visualize sample images and their corresponding masks.\n",
    "- Build a U-Net model with detailed comments.\n",
    "- Train the model.\n",
    "- Evaluate the model's performance and visualize the predictions.\n",
    "- Explore further improvements like data augmentation and additional metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## Import Libraries\n",
    "\n",
    "First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522664c8-6ad2-47b8-8703-b67016c1c91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and TensorFlow Datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Import Matplotlib for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Keras layers and models for building the U-Net\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba54cb-2ea4-4564-b2e4-deb91ba60c80",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the Oxford-IIIT Pet Dataset\n",
    "We will use TensorFlow Datasets (TFDS) to load the Oxford-IIIT Pet Dataset, which includes images of pets and their corresponding segmentation masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33f591b-0784-40e1-9cb2-baace5c37466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with info\n",
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True, data_dir=\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ed116d-b56f-4038-99f5-56af79c54098",
   "metadata": {},
   "source": [
    "- data_dir=\"./data/\" specifies the directory where the dataset will be stored.\n",
    "- with_info=True returns the dataset info, which contains metadata like the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cea9f88-fce1-4179-a4b0-78fd95ac40fb",
   "metadata": {},
   "source": [
    "---\n",
    "## Explore the Raw Mask Data\n",
    "Before preprocessing, let's examine the raw segmentation masks to understand their structure and unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791522db-1915-4fde-8ff6-5e3696c9f7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the raw mask: [2 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Access the training split of the dataset\n",
    "raw_train_dataset = dataset['train']\n",
    "\n",
    "# Iterate over one example to inspect the mask values\n",
    "for datapoint in raw_train_dataset.take(1):\n",
    "    # Extract the segmentation mask\n",
    "    raw_mask = datapoint['segmentation_mask']\n",
    "    # Get the unique values in the mask\n",
    "    unique_values_raw = tf.unique(tf.reshape(raw_mask, [-1])).y.numpy()\n",
    "    print(\"Unique values in the raw mask:\", unique_values_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415eefa3-d06a-4477-ac41-6a379a0f89e0",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "- The raw masks contain pixel values ranging from 0 to 3:\n",
    "    - 0: Background\n",
    "    - 1: Pet (foreground)\n",
    "    - 2: Border/outline\n",
    "    - 3: Not used in our case\n",
    "\n",
    "Our goal is to simplify the masks to have only two classes: background (0) and pet (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353c101-4de2-4e0b-9e51-ed0c9c42b65c",
   "metadata": {},
   "source": [
    "---\n",
    "## Set Image Size\n",
    "We define the image size to which all images and masks will be resized. Adjust IMG_SIZE based on your computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a469ac-58b0-4c7d-8800-abc2b4335398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size for resizing\n",
    "IMG_SIZE = 128  # You can adjust this value (e.g., 128, 256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24defe-f2bd-415f-be75-c9854d360dad",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Preprocessing Functions\n",
    "### Normalize Function\n",
    "The normalize function performs the following tasks:\n",
    "\n",
    "Converts input images to float values in the range [0, 1].\n",
    "Adjusts the masks to have only two values: 0 (background) and 1 (pet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53020434-b3c7-4f34-91d2-810b33c4f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "    \"\"\"Normalize images to [0,1] and adjust masks to have values 0 and 1.\"\"\"\n",
    "    # Convert image to float32 and scale from [0, 255] to [0, 1]\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    # Ensure mask is of type int32\n",
    "    input_mask = tf.cast(input_mask, tf.int32)\n",
    "    \n",
    "    # Map label 2 (border/outline) and label 3 to label 1 (foreground)\n",
    "    input_mask = tf.where(input_mask == 2, 1, input_mask)\n",
    "    input_mask = tf.where(input_mask == 3, 1, input_mask)\n",
    "    \n",
    "    # Ensure that background remains 0, and foreground is 1\n",
    "    input_mask = tf.where(input_mask != 1, 0, input_mask)\n",
    "    \n",
    "    return input_image, input_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee046cf-137f-4d6d-bec9-e8a08570c7b2",
   "metadata": {},
   "source": [
    "### Load and Preprocess Training Images\n",
    "The load_image_train function:\n",
    "\n",
    "Resizes images and masks to the defined IMG_SIZE.\n",
    "Applies random data augmentation (horizontal flip).\n",
    "Normalizes the images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8487d347-c763-431a-8f13-2602b1ee7a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_train(datapoint):\n",
    "    \"\"\"Preprocess training images and masks with augmentation.\"\"\"\n",
    "    # Resize the input image and mask\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(\n",
    "        datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE), method='nearest')\n",
    "\n",
    "    # Data augmentation: Random horizontal flip\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        # Flip image and mask horizontally\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    # Normalize the image and adjust the mask labels\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b218ce-e19c-4f9e-8e63-446107b7f82b",
   "metadata": {},
   "source": [
    "### Load and Preprocess Test Images\n",
    "The load_image_test function:\n",
    "\n",
    "Resizes images and masks to the defined IMG_SIZE.\n",
    "Normalizes the images and masks (without augmentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5393491-0e03-4f73-ac71-9e3984b04280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_test(datapoint):\n",
    "    \"\"\"Preprocess test images and masks without augmentation.\"\"\"\n",
    "    # Resize the input image and mask\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(\n",
    "        datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE), method='nearest')\n",
    "\n",
    "    # Normalize the image and adjust the mask labels\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d893e-4744-4c79-90cd-61795d8ced47",
   "metadata": {},
   "source": [
    "---\n",
    "## Prepare the Dataset for Training and Testing\n",
    "We now apply the preprocessing functions to the training and test datasets and prepare them for batching and shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879e00d4-22da-4e3c-ba3d-fc82c7d546fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "TRAIN_LENGTH = info.splits['train'].num_examples  # Number of training examples\n",
    "BATCH_SIZE = 16                                   # Batch size\n",
    "BUFFER_SIZE = 1000                                # Buffer size for shuffling\n",
    "\n",
    "# Prepare training dataset\n",
    "train_dataset = dataset['train'].map(\n",
    "    load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.cache()             # Cache the dataset for performance\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.repeat()            # Repeat the dataset indefinitely\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Prepare test dataset\n",
    "test_dataset = dataset['test'].map(load_image_test)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eb0a85-0f34-43fd-bf62-50c61c5deeb7",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify the Data Preprocessing\n",
    "Let's check the unique values in the masks after normalization to ensure that they only contain 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb074e70-954e-44b5-a83c-c67742b1156c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in the mask after normalization: [1]\n"
     ]
    }
   ],
   "source": [
    "# Check unique values in the masks after preprocessing\n",
    "for image, mask in train_dataset.take(1):\n",
    "    unique_values_after = tf.unique(tf.reshape(mask, [-1])).y.numpy()\n",
    "    print(\"Unique values in the mask after normalization:\", unique_values_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d933e6-2b79-4331-aa53-f1f856165269",
   "metadata": {},
   "source": [
    "Also, let's check the shapes and data types of images and masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dd2e260-1611-4225-a728-a322e194812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (16, 128, 128, 3)\n",
      "Image dtype: <dtype: 'float32'>\n",
      "Mask shape: (16, 128, 128, 1)\n",
      "Mask dtype: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# Inspect shapes and data types\n",
    "for image, mask in train_dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)        # Should be (BATCH_SIZE, IMG_SIZE, IMG_SIZE, 3)\n",
    "    print(\"Image dtype:\", image.dtype)        # Should be float32\n",
    "    print(\"Mask shape:\", mask.shape)          # Should be (BATCH_SIZE, IMG_SIZE, IMG_SIZE, 1)\n",
    "    print(\"Mask dtype:\", mask.dtype)          # Should be int32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5af77df-537e-495d-a602-4ac3f34590ba",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualize Sample Images and Masks\n",
    "Let's visualize a sample image and its corresponding mask to ensure that the preprocessing steps are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9e45766-8e50-4809-87ac-a43d72de2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(display_list):\n",
    "    \"\"\"Display image and mask side by side.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    title = ['Input Image', 'True Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        # Display the image\n",
    "        plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d32a2b-71be-4e66-b1cc-126acdee8056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
